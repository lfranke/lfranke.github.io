
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>VET: Visual Error Tomography</title>

    <meta name="description" content="Improving point-based novel view synthesis quality by completing point cloud with 3D error volumes from 2D error maps." />
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:title" content="VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering" />
    <meta property="og:description" content="Improving point-based novel view synthesis quality by completing point cloud with 3D error volumes from 2D error maps." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering" />
    <meta name="twitter:description" content="Improving point-based novel view synthesis quality by completing point cloud with 3D error volumes from 2D error maps." />
    <meta name="twitter:image" content="https://lfranke.github.io/vet/img/vet_teaser.png" />


<!--
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
//-->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>

    <link rel="stylesheet" href="css/dics.min.css">
    <script src="js/dics.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', domReady);
        function domReady() {
            for (const e of document.querySelectorAll(".b-dics")) {
                new Dics({
                    container: e,
                    textPosition: "top"
                });
            }
        }
    </script>
</head>

<body>
    <div class="container" id="main">
        <a href="..">Back to my projects</a>
        <br><br>
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>VET</b>: Visual Error Tomography for Point Cloud Completion</br> and High-Quality Neural Rendering</br> 
                <small>
                    Siggraph Asia 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>                        
                        <a style="text-decoration:none" href="https://lfranke.github.io">
                            Linus Franke
                          </a>
                        <br>FAU Erlangen-Nürnberg<br> &zwnj; 
                        
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://www.lgdv.tf.fau.de/person/darius-rueckert/">
                            Darius Rückert
                        </a>
                        <br>FAU Erlangen-Nürnberg
                        <br>Voxray GmbH
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://www.lgdv.tf.fau.de/person/laura-fink/">
                            Laura Fink
                        </a>

                        <br>FAU Erlangen-Nürnberg
                        <br>Fraunhofer IIS
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://www.lgdv.tf.fau.de/person/matthias-innmann/">
                            Matthias Innmann
                        </a>
                        <br>NavVis GmbH
                        <br>  &zwnj; 
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://www.lgdv.tf.fau.de/person/marc-stamminger/">
                            Marc Stamminger                          
                        </a>
                        <br>FAU Erlangen-Nürnberg
                        <br>  &zwnj; 
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2311.04634">
                            <image src="img/paper_text.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/adH6GyqC4Jk">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/lfranke/vet">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/playground_low.mp4" type="video/mp4" />
                </video>
						</div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    In the last few years, deep neural networks opened the doors for big advances in novel view synthesis. Many of these approaches are based on a (coarse) proxy geometry obtained by structure from motion algorithms. Small deficiencies in this proxy can be fixed by neural rendering, but larger holes or missing parts, as they commonly appear for thin structures or for glossy regions, still lead to distracting artifacts and temporal instability. In this paper, we present a novel neural-rendering-based approach to detect and fix such deficiencies. As a proxy, we use a point cloud, which allows us to easily remove outlier geometry and to fill in missing geometry without complicated topological operations.
                    
                    <br>Keys to our approach are (i) a differentiable, blending point-based renderer that can blend out redundant points, as well as (ii) the concept of <i>Visual Error Tomography</i> (VET), which allows us to lift 2D error maps to identify 3D-regions lacking geometry and to spawn novel points accordingly. Furthermore, (iii) by adding points as nested environment maps, our approach allows us to generate high-quality renderings of the surroundings in the same pipeline. In our results, we show that our approach can improve the quality of a point cloud obtained by structure from motion and thus increase novel view synthesis quality significantly. In contrast to point growing techniques, the approach can also fix large-scale holes and missing thin structures effectively. Rendering quality outperforms state-of-the-art methods and temporal stability is significantly improved, while rendering is possible at real-time frame rates.                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube-nocookie.com/embed/adH6GyqC4Jk" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Pipeline
                </h3>
                Overview of our neural rendering pipeline with visual error tomography. 
                <br>
                <br>
                <img src="img/pipeline_rs.jpg" style="width:100%">
                <p class="text-justify">
                    <br>

                    Point descriptors, positions and opacities are optimized via backpropagation, which automatically removes outliers during training. After initial convergence, our VET is used to predict missing 3D points from the error images. Following, the optimization is resumed until the next convergence. This process is repeated 1-3 times depending on scene complexity.        </p>

            </div>
        </div>
        <br>

        <div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Result: Point Cloud Completion and Rendering
                </h3>
                Intial point clouds are missing details, thus rendering is impacted heavily. Our pipeline produces convincing renderings through point cloud completion.
                <br>
                <div class="b-dics" style="width: 95%">
                <!--; float: left;">//-->
                    <img src="img/tree/out_debug_2_initial.jpg" alt="Initial Point Cloud" />
                    <img src="img/tree/out_debug_2_vet.jpg" alt="Ours" />
                </div>
                <small>(click and drag to swipe)</small>
                <div class="b-dics" style="width: 95%">
                    <img src="img/tree/init_render.jpg" alt="Initial Rendering" />
                    <img src="img/tree/vet_render.jpg" alt="Ours" />
                </div>
                <small>(click and drag to swipe)</small>
            </div>
        </div>
        <div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Result: Dense Point Cloud Completion and Cleaning
                </h3>
                Garden [Barron 2022]
                <video class="video" width=100% id="pointclouds" loop playsinline autoplay muted src="img/garden_init_vs_vet_low.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas height=0 class="videoMerge" id="pointcloudsMerge"></canvas>
                <p class="text-justify">
                    The initial COLMAP reconstruction is missing fine details and has outliers through fuzzy edges. Completion via VET and cleaning via opacity optimization fixes that.
                </p>

            </div>
        </div>
        <br>

        <div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <div class="b-dics" style="width: 100%">
                    <img src="img/comp/adop_crop.jpg" alt="ADOP Rendering" />
                    <img src="img/comp/vet_crop.jpg" alt="Ours" />
                </div>
                <small>(click and drag to swipe)</small>
            </div>
            <br>

        </div>

        <br>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Result: Sparse Point Cloud Completion
                </h3>
                COLMAPs sparse reconstruction can be efficiently completed.
                <video class="video" width=100% id="sparsepc" loop playsinline autoplay muted src="img/garden_sparse_complete_low.mp4" onplay="resizeAndPlay(this)"></video>
                <canvas height=0 class="videoMerge" id="sparsepcMerge"></canvas>
                <p class="text-justify">
                </p>

            </div>
        </div>



        <div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Result: Rendering
                </h3>
                <br>
                Playground [Knapitsch 2017]
                <div class="b-dics" style="width: 95%">
                <!--; float: left;">//-->
                <img src="img/tnt/pg_adop.jpg" alt="ADOP" />
                    <img src="img/tnt/pg_vet.jpg" alt="Ours" />
                    <img src="img/tnt/pg_mipnerf.jpg" alt="MipNeRF-360" />
                    <img src="img/tnt/pg_gt.jpg" alt="Ground Truth" />
                </div>
                <small>(click and drag to swipe)</small>
                <br>
                <br>
                Lighthouse [Knapitsch 2017]
                <div class="b-dics" style="width: 95%">
                <!--; float: left;">//-->
                <img src="img/tnt/lh_gt.jpg" alt="Ground Truth" />
                <img src="img/tnt/lh_mipnerf.jpg" alt="MipNeRF-360" />
                <img src="img/tnt/lh_vet.jpg" alt="Ours" />
                <img src="img/tnt/lh_adop.jpg" alt="ADOP" />
                </div>
                <small>(click and drag to swipe)</small>
            </div>
        </div>
        <br>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{franke2023vet,
    title={VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering},
    author={Linus Franke and Darius R{\"u}ckert and 
            Laura Fink and Matthias Innmann and Marc Stamminger},
    booktitle = {ACM SIGGRAPH Asia 2023 Conference Proceedings},
    year = {2023}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                The authors thank the anonymous reviewers for their valuable feedback and Lukas Meyer for the Cherry Tree scene.
                Additionally, we thank Stefan Romberg, Michael Gerstmayr and Tim Habigt for the fruitful discussions as well as NavVis GmbH for providing datasets for testing.
                <br>
                Linus Franke was supported by the Bayerische Forschungsstiftung (Bavarian Research Foundation) AZ-1422-20.
                The authors gratefully acknowledge the scientific support and HPC resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) under the NHR project b162dc. NHR funding is provided by federal and Bavarian state authorities. NHR@FAU hardware is partially funded by the German Research Foundation (DFG) – 440719683.
                <br>
                <br>

                <p class="text-justify">
                    The website template was adapted from <a href="https://jonbarron.info/zipnerf/">Zip-NeRF</a>, who borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                    Image sliders are from <a href="https://bakedsdf.github.io/">BakedSDF</a>.
                </p>
            </div>
        </div>

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                References
            </h3>
    <div class="content has-text-justified">
        <p>
          [Müller 2022] Müller, T., Evans, A., Schied, C. and Keller, A., 2022. Instant neural graphics primitives with a multiresolution hash encoding
        </p>
        <p>
          [Rückert 2022] Rückert, D., Franke L., and Stamminger M. 2022. "Adop: Approximate differentiable one-pixel point rendering." ACM Transactions on Graphics (ToG) 41.4 (2022): 1-14.
        </p>
        <p>
          [Barron 2022] Barron, Jonathan T., et al. "Mip-nerf 360: Unbounded anti-aliased neural radiance fields." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
          </p>  

            <p>
              [Knapitsch 2017] Knapitsch, Arno, et al. "Tanks and temples: Benchmarking large-scale scene reconstruction." ACM Transactions on Graphics (ToG) 36.4 (2017): 1-13.
            </p>
      </div>
    </div>

</body>
</html>
