
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting</title>

    <meta name="description" content="In this paper, we complete LiDAR point clouds with photometric Gaussian splatting reconstruction." />
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://lfranke.github.io/surffill/media/teaserimage.jpg">
    <meta property="og:image:type" content="image/jpg">
    <!-- <meta property="og:video:width" content="750">
    <meta property="og:image:height" content="500"> -->
    <meta property="og:type" content="website" />
    <meta property="og:title" content="SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting" />
    <meta property="og:description" content="In this paper, we complete LiDAR point clouds with photometric Gaussian splatting reconstruction." />

    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="https://lfranke.github.io/surffill/" />
    <meta name="twitter:title" content="SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting" />
    <meta name="twitter:description" content="In this paper, we complete LiDAR point clouds with photometric Gaussian splatting reconstruction." />
    <meta name="twitter:image" content="https://lfranke.github.io/surffill/media/teaserimage.jpg" />


<!--
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
//-->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>

    <link rel="stylesheet" href="css/dics.min.css">
    <script src="js/dics.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', domReady);
        function domReady() {
            for (const e of document.querySelectorAll(".b-dics")) {
                new Dics({
                    container: e,
                    textPosition: "top"
                });
            }
        }
    </script>
</head>

<body>
    <div class="container" id="main">
        <a href="..">Back to my projects</a>
        <br><br>
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>SurfFill</b>: <br> Completion of LiDAR Point Clouds via Gaussian Surfel Splatting<br>
                <small>
                arXiv                
            </small>

            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a style="text-decoration:none" href="https://www.linkedin.com/in/svenja-strobel-370a27332/">
                        Svenja Strobel
                        </a>
                        <br>FAU Erlangen-Nürnberg<br>
                        NavVis GmbH &zwnj;
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://www.lgdv.tf.fau.de/person/matthias-innmann/">
                        Matthias Innmann
                        </a>
                        <br>NavVis GmbH<br> &zwnj;
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://eggerbernhard.ch/">
                        Bernhard Egger
                        </a>
                        <br>FAU Erlangen-Nürnberg<br>
                         &zwnj;
                    </li>
                     <li>
                        <a style="text-decoration:none" href="https://www.lgdv.tf.fau.de/person/marc-stamminger/">
                            Marc Stamminger
                        </a>
                        <br>FAU Erlangen-Nürnberg
                        <br>  &zwnj;
                    </li>
                    <li>
                        <a style="text-decoration:none" href="https://lfranke.github.io">
                            Linus Franke
                        </a>
                        <br>FAU Erlangen-Nürnberg<br> 
                        Inria, Université Côte d'Azur&zwnj;
                    </li>
                   
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <img src="media/paper.jpg" height="60px">
                                <h4><strong>Paper (coming soon)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/OS3q5OWT-sg">
                            <image src="media/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                         <li>
                            <a href="">
                            <image src="media/github.png" height="60px">
                                <h4><strong>Code (coming soon)</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>




        <div class="row">


            <div class="col-md-8 col-md-offset-2">
                <br>
                <img src="media/sf_header.jpg" style="width:100%">
                With our method, an incomplete LiDAR point cloud (left) is completed (right) via photometric Gaussian reconstruction.
                <p class="text-justify">
                <br>
                    <h4>
                        Key Insights
                    </h4>
                <h5>
                </h5>
                    <p class="text-justify">
                        <ol>
                            <li>
                    <b>LiDAR scans</b>, while globally precise, often <b>miss thin structures</b>, edges, and dark or absorbent materials, which are regions where photometric reconstruction excels. 
                            </li>
                            <li>
                    Conversely, <b>image-based reconstruction</b> methods are <b>highly detailed in feature-rich regions</b> but lack LiDAR's accuracy in smooth, textureless areas. 
                            </li>
                            <li>
                    Our method bridges this gap by <b>combining the strengths of both</b> modalities.               
                            </li>
                        </ol> 
                    </p>
            </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    LiDAR-captured point clouds are often considered the gold standard in active 3D reconstruction. 
                    While their accuracy is exceptional in flat regions, the capturing is susceptible to miss small geometric structures and may fail with dark, absorbent materials.
                    Alternatively, capturing multiple photos of the scene and applying 3D photogrammetry can infer these details as they often represent feature-rich regions.
                    However, the accuracy of LiDAR for featureless regions is rarely reached.
                </p>
                <p class="text-justify">
                    Therefore, we suggest combining the strengths of LiDAR and camera-based capture by introducing SurfFill: a Gaussian surfel-based LiDAR completion scheme.
                    We analyze LiDAR capturings and attribute LiDAR beam divergence as a main factor for artifacts, manifesting mostly at thin structures and edges.
                    We use this insight to introduce an <b>ambiguity heuristic</b> for completed scans by evaluating the change in density in the point cloud.
                    This allows us to identify points close to missed areas, which we can then use to grow additional points from to complete the scan.
                    For this point growing, we constrain Gaussian surfel reconstruction [Huang et al. 2024] to focus optimization and densification on these ambiguous areas.
                    Finally, Gaussian primitives of the reconstruction in ambiguous areas are extracted and sampled for points to complete the point cloud.
                    To address the challenges of large-scale reconstruction, we extend this pipeline with a divide-and-conquer scheme for building-sized point cloud completion.
                    We evaluate on the task of LiDAR point cloud completion of synthetic and real-world scenes and find that our method outperforms previous reconstruction methods.                    </p>        

            </div>
        </div>

                <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube-nocookie.com/embed/OS3q5OWT-sg" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
        <br>

        <br>

        <br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Contributions
                </h3>
                <p class="text-justify">
                    <ol>
                        <li>
                An analysis of LiDAR artifacts and a novel <b>ambiguity heuristic</b> to identify regions likely adjacent to missing geometry.
                        </li>
                        <li>
                A <b>focused reconstruction</b> pipeline integrating Gaussian surfel splatting with constraints, losses, and sampling strategies tailored for ambiguous regions.
                        </li>
                        <li>
                        A procedure that extracts and samples Gaussian primitives to <b>synthesize missing geometry</b> with high fidelity.
                        </li>
                        <li>
                A divide-and-conquer framework enabling completion of building-scale LiDAR scans with tens of <b>millions of points</b>.                        
                        </li>
                    </ol> 
                </p>
            </div>
        </div>

<br><br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    LiDAR Artifacts
                </h3>
                <img id="test" src="media/lidar.jpg" width="100%">  
                <br><br>
                A LiDAR system works by emitting diverging laser pulses and measuring the time they take to return after reflecting off surfaces to compute precise 3D distances.                
                <br>
                <br>
                <ul>(a)
                    Flat surfaces are captured accurately.
                </ul>
                <ul>   
                    (b)
                    Thin structures (red) often fall within a beam's divergence and produce multiple different measurements (green), resulting in averaged samples in between surfaces (blue).
                </ul>
                LiDAR post-processing mostly remove ambigious samples, leaving incomplete, but accurate scans.
        </div>
        <br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Ambiguity Heuristic
                </h3>
                <img id="test" src="media/ambiguityheuristic.jpg" width="80%">  
                <br><br>
                Directly identifying missing samples is difficult, but we can estimate closeby areas by computing point densities, with we use as an <b>Ambiguity Heuristic</b>.

        </div>
        <br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Reconstruction Pipeline
                </h3>
                <img id="test" src="media/tablepipeline.jpg" width="100%">  
                <br><br>
                Based on the Ambiguity Heuristic, we preprocess (downsample) the LiDAR point cloud non-uniformly, and reconstruct missing areas photometrically with 2D Gaussians [Huang et al. 2024], adjusted and constrained to mainly reconstruct missing areas.
                <br>
                <br>
                Afterwards, we filter the Gaussian model only for missing areas in the original LiDAR scan and sample additional, synthetic points and combine them with the scan.
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Large Scenes
                                    </h3>
                <img id="test" src="media/largescenes_shadow.jpg" width="100%">  
                <br><br>
                <p>
                The handling of large scenes is shown with a room example: <br>
                a) a chunk (<b>green</b>) selects a subset of <b>cameras</b> (<b>red</b>) inside its bounds or viewing it; <br>
                b) the chunk gathers <b>points</b> in its bounding box (green), an extended region (blue), and camera frusta (pink); <br> 
                c) the final <b>chunk dataset</b>.
                </p>
        </div>
        <br>

         <div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Result: LiDAR completion
                </h3>
                <br>
                <div style="width: 100%; text-align: center;">
                    (Videos will play during mouse hovering / on touch on mobile version.)
                <video class="video" id="pointclouds" loop playsinline autoplay muted src="media/result_lidar_ours.mp4" ></video>
                <canvas class="videoMerge" id="pointcloudsMerge"></canvas>
               </div>
               <br>             
                <p class="text-justify">
                </p>

            </div>
        </div>
              <br>


         <div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Result: LiDAR Completion on ScanNet++
                </h3>
                <br>
                <div style="width: 100%; text-align: center;">
                </div>
                Comparing with 2DGS [Huang et al. 2024] and Gaussian Opacity Fields (GOF) [Yu et al. 2024] on ScanNet++ [Yeshwanth et al. 2023].
                <div style="display: flex; width: 100%;">

                    <div style="flex: 1; text-align: center;">
                        <video class="video" id="pointclouds2" 
                            loop playsinline autoplay muted 
                            src="media/result_scannet.mp4"
                            style="max-width: 100%; height: auto;">
                        </video>
                    </div>

                    <div style="flex: 1; text-align: center;">
                        <video class="video" id="pointclouds3" 
                            loop playsinline autoplay muted 
                            src="media/result_scannet_compare_others.mp4"
                            style="max-width: 100%; height: auto;">
                        </video>
                    </div>

                    </div>
               <br>             
                <p class="text-justify">
                </p>

            </div>
        </div>

        <div class="row comp-margin">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Result: LiDAR completion on Synthetic Attic Scene
                </h3>
                <br>
                Comparing with 2DGS [Huang et al. 2024]
                <div style="width: 100%; text-align: center;">
                    (Videos will play during mouse hovering / on touch on mobile version.)
                <video class="video" id="pointclouds4" loop playsinline autoplay muted src="media/results_attic.mp4" ></video>
                <canvas class="videoMerge" id="pointclouds4Merge"></canvas>
               </div>
               <br>             
                <p class="text-justify">
                </p>

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <hr>
            <pre><code>@article{strobel2025surffill,
  author      = {Svenja Strobel and Matthias Innmann and Bernhard Egger and 
                 Marc Stamminger and Linus Franke},
  title       = {{SurfFill}: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting},
  journal     = {ArXiv},
  month       = {Dez},
  year        = {2025},
  url         = {https://lfranke.github.io/surffill}
}</code></pre>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>

                <p class="text-justify">
                Our gratitude goes to Stefan Romberg, Michael Gerstmayr, and Tim Habigt for the productive discussions. 
                The authors gratefully acknowledge the scientific support and HPC resources provided by the National High Performance Computing Center  of the Friedrich-Alexander-Universität Erlangen-Nürnberg (NHR@FAU) under the project b162dc. NHR funding is provided by federal and Bavarian state authorities. NHR@FAU hardware is partially funded by the German Research Foundation (DFG) – 440719683. </p>
                <br>
                <br>
                

                <p class="text-justify">
                    The website template was adapted from <a href="https://lorafib.github.io/ref_depth">RefDepth</a>, who borrowed from  <a href="https://lfranke.github.io/vet">VET</a>, who borrowed from <a href="https://jonbarron.info/zipnerf/">Zip-NeRF</a>, who borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                </p>
            </div>
        </div>

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                References
            </h3>
    <div class="content has-text-justified">
        <p>    
            [Huang et al. 2024] Huang, B., Yu, Z., Chen, A., Geiger, A., & Gao, S. (2024, July). 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 conference papers (pp. 1-11).
        </p>
        <p>    
            [Yu et al. 2024] Yu, Z., Sattler, T., & Geiger, A. (2024). Gaussian opacity fields: Efficient adaptive surface reconstruction in unbounded scenes. ACM Transactions on Graphics (ToG), 43(6), 1-13.
        </p>
        <p>
            [Yeshwanth et al. 2023] Yeshwanth, C., Liu, Y. C., Nießner, M., & Dai, A. (2023). Scannet++: A high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 12-22).
        </p>
      </div>
    </div>

</body>
</html>
